"""
Script de r√©duction dimensionnelle des embeddings d'articles
Utilise PCA pour r√©duire de 250 dimensions √† 50-80 dimensions
pour optimiser l'espace de stockage Azure
"""

import pickle
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import os
import sys
from datetime import datetime


def load_embeddings(filepath: str) -> np.ndarray:
    """
    Charge le fichier embeddings original (matrice numpy)

    Args:
        filepath: Chemin vers le fichier pickle

    Returns:
        Matrice numpy des embeddings
    """
    print(f"üìÇ Chargement des embeddings depuis {filepath}...")

    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Fichier non trouv√©: {filepath}")

    with open(filepath, 'rb') as f:
        embeddings = pickle.load(f)

    # V√©rifier que c'est bien un numpy array
    if not isinstance(embeddings, np.ndarray):
        raise TypeError(f"Format inattendu: {type(embeddings)}")

    print(f"‚úÖ Embeddings charg√©s")
    print(f"üìä Structure: Matrice {embeddings.shape[0]} articles √ó {embeddings.shape[1]} dimensions")
    print(f"   Taille en m√©moire: {embeddings.nbytes / 1024 / 1024:.2f} MB")

    return embeddings


def reduce_dimensions(matrix: np.ndarray, n_components: int = 80,
                     random_state: int = 42) -> tuple:
    """
    Applique PCA pour r√©duire les dimensions

    Args:
        matrix: Matrice des embeddings originaux
        n_components: Nombre de dimensions cibles
        random_state: Seed pour reproductibilit√©

    Returns:
        Tuple (matrice_r√©duite, mod√®le_pca)
    """
    print(f"\nüéØ R√©duction PCA: {matrix.shape[1]} ‚Üí {n_components} dimensions")
    print("‚è≥ Cette op√©ration peut prendre quelques minutes...")

    # Initialiser PCA
    pca = PCA(n_components=n_components, random_state=random_state)

    # Appliquer la transformation
    reduced_matrix = pca.fit_transform(matrix)

    # Afficher les statistiques
    explained_variance = pca.explained_variance_ratio_.sum()

    print(f"\n‚úÖ R√©duction termin√©e!")
    print(f"üìä Statistiques PCA:")
    print(f"   - Variance expliqu√©e: {explained_variance * 100:.2f}%")
    print(f"   - Dimensions: {matrix.shape[1]} ‚Üí {reduced_matrix.shape[1]}")
    print(f"   - R√©duction: {(1 - reduced_matrix.shape[1] / matrix.shape[1]) * 100:.1f}%")
    print(f"   - Taille avant: {matrix.nbytes / 1024 / 1024:.2f} MB")
    print(f"   - Taille apr√®s: {reduced_matrix.nbytes / 1024 / 1024:.2f} MB")
    print(f"   - √âconomie: {(1 - reduced_matrix.nbytes / matrix.nbytes) * 100:.1f}%")

    return reduced_matrix, pca


def save_reduced_embeddings(embeddings_matrix: np.ndarray, pca_model: PCA,
                           output_path: str, pca_path: str):
    """
    Sauvegarde les embeddings r√©duits et le mod√®le PCA

    Args:
        embeddings_matrix: Matrice numpy des embeddings r√©duits
        pca_model: Mod√®le PCA entra√Æn√©
        output_path: Chemin de sauvegarde des embeddings
        pca_path: Chemin de sauvegarde du mod√®le PCA
    """
    print(f"\nüíæ Sauvegarde des r√©sultats...")

    # Sauvegarder les embeddings r√©duits (matrice numpy)
    with open(output_path, 'wb') as f:
        pickle.dump(embeddings_matrix, f, protocol=pickle.HIGHEST_PROTOCOL)

    file_size_mb = os.path.getsize(output_path) / 1024 / 1024
    print(f"‚úÖ Embeddings r√©duits sauvegard√©s: {output_path}")
    print(f"   Taille du fichier: {file_size_mb:.2f} MB")

    # Sauvegarder le mod√®le PCA (pour r√©f√©rence)
    with open(pca_path, 'wb') as f:
        pickle.dump(pca_model, f, protocol=pickle.HIGHEST_PROTOCOL)

    pca_size_mb = os.path.getsize(pca_path) / 1024 / 1024
    print(f"‚úÖ Mod√®le PCA sauvegard√©: {pca_path}")
    print(f"   Taille du fichier: {pca_size_mb:.2f} MB")


def test_quality(original_matrix: np.ndarray, reduced_matrix: np.ndarray,
                n_samples: int = 100):
    """
    Teste la qualit√© de la r√©duction en comparant les similarit√©s

    Args:
        original_matrix: Matrice des embeddings originaux
        reduced_matrix: Matrice des embeddings r√©duits
        n_samples: Nombre d'√©chantillons √† tester
    """
    print(f"\nüß™ Test de qualit√© sur {n_samples} √©chantillons...")

    from sklearn.metrics.pairwise import cosine_similarity

    # Limiter le nombre d'√©chantillons si n√©cessaire
    n_samples = min(n_samples, original_matrix.shape[0])
    n_comparison = min(200, original_matrix.shape[0])

    correlations = []

    # √âchantillonner al√©atoirement
    np.random.seed(42)
    sample_indices = np.random.choice(original_matrix.shape[0], n_samples, replace=False)

    for idx in sample_indices:
        # Vecteurs de l'article √† tester
        orig_vec = original_matrix[idx:idx+1]  # Garder dimension (1, n)
        red_vec = reduced_matrix[idx:idx+1]

        # Prendre un √©chantillon de comparaison
        comparison_indices = np.random.choice(original_matrix.shape[0], n_comparison, replace=False)

        # Calculer similarit√©s cosinus
        orig_sims = cosine_similarity(orig_vec, original_matrix[comparison_indices])[0]
        red_sims = cosine_similarity(red_vec, reduced_matrix[comparison_indices])[0]

        # Corr√©lation entre les similarit√©s
        correlation = np.corrcoef(orig_sims, red_sims)[0, 1]
        correlations.append(correlation)

    avg_correlation = np.mean(correlations)

    print(f"‚úÖ Test termin√©!")
    print(f"üìä Corr√©lation moyenne des similarit√©s: {avg_correlation:.4f}")

    if avg_correlation > 0.95:
        print("üéâ Excellente qualit√©! La r√©duction pr√©serve tr√®s bien les relations.")
    elif avg_correlation > 0.90:
        print("üëç Bonne qualit√©. La r√©duction est acceptable.")
    else:
        print("‚ö†Ô∏è  Qualit√© moyenne. Envisager d'augmenter le nombre de composantes.")

    return avg_correlation


def main():
    """Fonction principale"""
    print("=" * 70)
    print("üî¨ R√âDUCTION DIMENSIONNELLE DES EMBEDDINGS D'ARTICLES")
    print("=" * 70)
    print(f"‚è∞ D√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Configuration
    INPUT_PATH = "data/articles_embeddings.pickle"
    OUTPUT_PATH = "data/articles_embeddings_reduced.pickle"
    PCA_MODEL_PATH = "models/pca_model.pkl"
    N_COMPONENTS = 80  # R√©duction √† 80 dimensions (ajustable)

    # Cr√©er le dossier models si n√©cessaire
    os.makedirs("models", exist_ok=True)

    try:
        # √âtape 1: Charger les embeddings originaux (matrice numpy)
        embeddings_original = load_embeddings(INPUT_PATH)

        # √âtape 2: Appliquer PCA
        reduced_matrix, pca_model = reduce_dimensions(
            embeddings_original,
            n_components=N_COMPONENTS
        )

        # √âtape 3: Sauvegarder
        save_reduced_embeddings(
            reduced_matrix,
            pca_model,
            OUTPUT_PATH,
            PCA_MODEL_PATH
        )

        # √âtape 4: Tester la qualit√©
        correlation = test_quality(embeddings_original, reduced_matrix)

        # R√©sum√© final
        print("\n" + "=" * 70)
        print("‚úÖ R√âDUCTION TERMIN√âE AVEC SUCC√àS!")
        print("=" * 70)

        original_size = os.path.getsize(INPUT_PATH) / 1024 / 1024
        reduced_size = os.path.getsize(OUTPUT_PATH) / 1024 / 1024

        print(f"\nüì¶ R√©sum√©:")
        print(f"   Fichier original:  {original_size:.2f} MB")
        print(f"   Fichier r√©duit:    {reduced_size:.2f} MB")
        print(f"   √âconomie:          {original_size - reduced_size:.2f} MB ({(1 - reduced_size/original_size)*100:.1f}%)")
        print(f"   Qualit√© (corr.):   {correlation:.4f}")
        print(f"\nüí° Le fichier r√©duit peut maintenant √™tre utilis√© avec Azure Functions!")
        print(f"\n‚è∞ Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
