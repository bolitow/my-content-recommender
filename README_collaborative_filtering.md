# Syst√®me de Recommandation Collaborative Filtering

## Vue d'ensemble

Ce projet impl√©mente trois algorithmes de filtrage collaboratif (Collaborative Filtering) pour recommander des articles d'actualit√©. Le syst√®me exploite les interactions utilisateur-article pour identifier des patterns de pr√©f√©rences et g√©n√©rer des recommandations personnalis√©es bas√©es sur les comportements collectifs.

## Architecture des Approches

### üéØ Approche 1: SVD (Singular Value Decomposition)

**Principe:** D√©composition matricielle par factorisation en valeurs singuli√®res pour capturer les facteurs latents des pr√©f√©rences utilisateurs et caract√©ristiques des articles.

**Fonctionnement:**
1. **Factorisation matricielle:** D√©compose la matrice R (utilisateur-article) en trois matrices
2. **Optimisation:** Minimisation de l'erreur de reconstruction avec r√©gularisation
3. **Pr√©diction:** Calcul du produit des facteurs latents pour estimer les ratings manquants

**Formulation math√©matique:**

Variables :
- `R` : Matrice de ratings utilisateur-article
- `U` : Matrice des facteurs latents utilisateurs (n_users √ó n_factors)
- `Œ£` : Matrice diagonale des valeurs singuli√®res
- `V^T` : Matrice des facteurs latents articles (n_factors √ó n_items)
- `p_u` : Vecteur de facteurs latents pour l'utilisateur u
- `q_i` : Vecteur de facteurs latents pour l'article i
- `Œº` : Moyenne globale des ratings
- `b_u` : Biais de l'utilisateur u
- `b_i` : Biais de l'article i
- `Œª` : Param√®tre de r√©gularisation (reg_all = 0.02)
- `Œ±` : Taux d'apprentissage (lr_all = 0.005)

√âquations :
```
# D√©composition SVD
R ‚âà U √ó Œ£ √ó V^T

# Pr√©diction de rating
rÃÇ_ui = Œº + b_u + b_i + p_u ¬∑ q_i

# Fonction objectif √† minimiser
L = Œ£(r_ui - rÃÇ_ui)¬≤ + Œª(||p_u||¬≤ + ||q_i||¬≤ + b_u¬≤ + b_i¬≤)

# Mise √† jour par descente de gradient
p_u ‚Üê p_u + Œ±(e_ui √ó q_i - Œª √ó p_u)
q_i ‚Üê q_i + Œ±(e_ui √ó p_u - Œª √ó q_i)
b_u ‚Üê b_u + Œ±(e_ui - Œª √ó b_u)
b_i ‚Üê b_i + Œ±(e_ui - Œª √ó b_i)

o√π e_ui = r_ui - rÃÇ_ui (erreur de pr√©diction)
```

**Param√®tres du mod√®le:**
- `n_factors`: 100 (dimensions des facteurs latents)
- `n_epochs`: 20 (nombre d'it√©rations)
- `lr_all`: 0.005 (taux d'apprentissage)
- `reg_all`: 0.02 (r√©gularisation L2)

**Avantages:**
- Capture efficacement les patterns complexes dans les donn√©es
- Gestion robuste des donn√©es manquantes
- Bonne g√©n√©ralisation gr√¢ce √† la r√©gularisation

**Limitations:**
- Probl√®me de d√©marrage √† froid pour nouveaux utilisateurs/articles
- Sensible aux hyperparam√®tres
- Co√ªt computationnel √©lev√© pour grandes matrices

---

### üß† Approche 2: NMF (Non-Negative Matrix Factorization)

**Principe:** Factorisation matricielle non-n√©gative garantissant l'interpr√©tabilit√© des facteurs latents en imposant des contraintes de positivit√©.

**Fonctionnement:**
1. **D√©composition non-n√©gative:** Factorise R en deux matrices non-n√©gatives
2. **Optimisation it√©rative:** R√®gles de mise √† jour multiplicatives
3. **Interpr√©tation:** Les facteurs repr√©sentent des th√®mes/cat√©gories positifs

**Formulation math√©matique:**

Variables :
- `W` : Matrice des profils utilisateurs (n_users √ó n_factors), W ‚â• 0
- `H` : Matrice des profils articles (n_factors √ó n_items), H ‚â• 0
- `w_u` : Vecteur profil de l'utilisateur u
- `h_i` : Vecteur profil de l'article i
- `n_factors` : Nombre de facteurs latents (50)
- `Œµ` : Petite constante pour √©viter la division par z√©ro

√âquations :
```
# Approximation NMF
R ‚âà W √ó H
avec contrainte : W ‚â• 0, H ‚â• 0

# Pr√©diction
rÃÇ_ui = w_u ¬∑ h_i = Œ£_k(w_uk √ó h_ki)

# Fonction objectif (divergence de Kullback-Leibler)
D_KL = Œ£_ui[r_ui √ó log(r_ui/rÃÇ_ui) - r_ui + rÃÇ_ui]

# R√®gles de mise √† jour multiplicatives
W_uk ‚Üê W_uk √ó [Œ£_i(R_ui √ó H_ki / rÃÇ_ui)] / [Œ£_i H_ki]
H_ki ‚Üê H_ki √ó [Œ£_u(R_ui √ó W_uk / rÃÇ_ui)] / [Œ£_u W_uk]

# Gestion des utilisateurs/articles inconnus
Si utilisateur inconnu : utiliser moyenne des ratings de l'article
Si article inconnu : score bas√© sur popularit√©
```

**Param√®tres du mod√®le:**
- `n_factors`: 50 (dimensions r√©duites pour meilleure interpr√©tabilit√©)
- `n_epochs`: 50 (plus d'it√©rations pour convergence)

**Avantages:**
- Facteurs interpr√©tables (toujours positifs)
- Adapt√© aux donn√©es de comptage (clics)
- R√©sultats explicables

**Limitations:**
- Contrainte de non-n√©gativit√© peut limiter l'expressivit√©
- Convergence plus lente que SVD
- Performance g√©n√©ralement inf√©rieure √† SVD

---

### ‚ö° Approche 3: ALS (Alternating Least Squares)

**Principe:** Optimisation altern√©e des facteurs utilisateurs et articles pour les donn√©es implicites (clics), particuli√®rement efficace pour les matrices creuses.

**Fonctionnement:**
1. **Feedback implicite:** Conversion des clics en niveaux de confiance
2. **Optimisation altern√©e:** Fixe U pour optimiser V, puis fixe V pour optimiser U
3. **Pond√©ration:** Les clics multiples indiquent une pr√©f√©rence plus forte

**Formulation math√©matique:**

Variables :
- `C_ui` : Matrice de confiance (bas√©e sur le nombre de clics)
- `P_ui` : Matrice de pr√©f√©rence binaire (1 si cliqu√©, 0 sinon)
- `X_u` : Facteurs latents utilisateur
- `Y_i` : Facteurs latents article
- `Œ±` : Param√®tre de confiance (40)
- `Œª` : R√©gularisation (0.01)
- `r_ui` : Nombre de clics utilisateur u sur article i

√âquations :
```
# Matrice de confiance
C_ui = 1 + Œ± √ó r_ui

# Matrice de pr√©f√©rence
P_ui = 1 si r_ui > 0, 0 sinon

# Fonction objectif
L = Œ£_ui C_ui(P_ui - X_u^T √ó Y_i)¬≤ + Œª(||X_u||¬≤ + ||Y_i||¬≤)

# Optimisation altern√©e (ferm√©e analytiquement)
Fixe Y, r√©sout pour X:
X_u = (Y^T √ó C^u √ó Y + ŒªI)^(-1) √ó Y^T √ó C^u √ó P^u

Fixe X, r√©sout pour Y:
Y_i = (X^T √ó C^i √ó X + ŒªI)^(-1) √ó X^T √ó C^i √ó P^i

o√π C^u est la matrice diagonale des confiances pour l'utilisateur u

# Score de recommandation
score(u,i) = X_u^T √ó Y_i
```

**Param√®tres du mod√®le:**
- `factors`: 100 (dimensions des facteurs latents)
- `regularization`: 0.01 (r√©gularisation L2)
- `iterations`: 20 (nombre d'alternances)
- `alpha`: 40 (param√®tre de confiance)

**Impl√©mentation avec matrices creuses:**
```python
# Utilisation de lil_matrix pour construction efficace
user_item_matrix = lil_matrix((n_users, n_items))
# Conversion en csr_matrix pour calculs rapides
user_item_matrix = user_item_matrix.tocsr()
item_user_matrix = user_item_matrix.T.tocsr()
```

**Avantages:**
- Tr√®s efficace pour donn√©es implicites
- Excellent pour matrices tr√®s creuses (~99.8% de sparsit√©)
- Solution analytique √† chaque √©tape
- Meilleure performance empirique du notebook

**Limitations:**
- Suppose lin√©arit√© des interactions
- N√©cessite tuning du param√®tre Œ±
- Cold start problem persistant

---

## Pr√©paration des Donn√©es

### Traitement des Interactions

**Calcul du score d'engagement:**
```python
# Score composite bas√© sur clics et temps
max_time = interactions['avg_time_to_click'].max()
engagement_score = click_count √ó (1 - avg_time_to_click / max_time)

# Normalisation en rating 1-5
rating = 1 + 4 √ó (engagement_score - min) / (max - min)
```

**Statistiques du dataset:**
- Articles totaux: 363,946
- Articles cliqu√©s: 20,793
- Utilisateurs uniques: 210,991
- Interactions totales: 1,176,699
- Rating moyen: 1.23
- Sparsit√© de la matrice: ~99.8%

### Split Temporel Am√©lior√©

**Strat√©gie de split:**
```python
def improved_temporal_split(interactions, test_ratio=0.2):
    # Pour chaque utilisateur:
    # - 80% premi√®res interactions ‚Üí train
    # - 20% derni√®res interactions ‚Üí test
    # - Garantit min 2 interactions dans train
    # - Filtre test pour articles pr√©sents dans train
```

**R√©sultats du split:**
- Train: 618,118 interactions
- Test: 208,472 interactions
- Utilisateurs dans test: 78,500
- Articles testables: 3,493

---

## M√©triques d'√âvaluation

### M√©triques Utilis√©es

**Hit@K:** Utilisateur a au moins une bonne recommandation dans le TOP K
```
Hit@K = 1 si |reco[:K] ‚à© actual| > 0, 0 sinon
```

**Precision@K:** Proportion de recommandations correctes
```
Precision@K = |reco[:K] ‚à© actual| / K
```

**Recall@K:** Proportion d'articles pertinents retrouv√©s
```
Recall@K = |reco[:K] ‚à© actual| / |actual|
```

**F1@K:** Moyenne harmonique precision-recall
```
F1@K = 2 √ó (Precision@K √ó Recall@K) / (Precision@K + Recall@K)
```

### R√©sultats Exp√©rimentaux

| Mod√®le | Hit@5 | Hit@10 | Hit@20 | Precision@10 | Recall@10 | F1@10 |
|--------|-------|--------|--------|--------------|-----------|-------|
| **SVD** | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.0000 |
| **NMF** | 0.004 | 0.018 | 0.046 | 0.002 | 0.008 | 0.0032 |
| **ALS** | 0.064 | 0.190 | 0.256 | 0.020 | 0.091 | 0.0328 |

**Meilleur mod√®le:** ALS (F1@10 = 0.0328)

### Analyse des R√©sultats

**Performance par mod√®le:**
- **ALS:** 25.6% des utilisateurs ont au moins un hit @20
- **NMF:** 4.6% des utilisateurs ont au moins un hit @20
- **SVD:** 0% de hits (probl√®me potentiel de configuration)

**Insights:**
- Les m√©triques faibles refl√®tent la nature √©ph√©m√®re des articles d'actualit√©
- ALS surperforme gr√¢ce √† sa gestion native du feedback implicite
- SVD n√©cessite probablement un ajustement d'hyperparam√®tres
- La sparsit√© extr√™me (99.8%) impacte tous les mod√®les

---

## Framework d'√âvaluation Robuste

### Gestion des Cas Limites

```python
class ImprovedRecommender:
    def recommend(self, user_id, n=10):
        # Cas 1: Utilisateur et article connus
        if user_known and item_known:
            return model.predict(user_id, item_id)

        # Cas 2: Utilisateur inconnu, article connu
        elif item_known:
            return global_mean + item_bias

        # Cas 3: Article inconnu
        else:
            return popularity_based_score
```

### Strat√©gies de Fallback

1. **√âchantillonnage intelligent des candidats:**
   - Top 250 articles populaires
   - 250 articles al√©atoires
   - Limite √† 500 candidats pour performance

2. **Scoring hybride pour articles inconnus:**
   ```python
   popularity_score = item_clicks / max_clicks
   fallback_score = global_mean √ó (0.5 + 0.5 √ó popularity_score)
   ```

3. **Recommandations de secours:**
   - Utilisateurs sans historique ‚Üí articles populaires
   - Erreurs de pr√©diction ‚Üí fallback vers popularit√©

---

## Optimisations Impl√©ment√©es

### Performance

- **Matrices creuses:** Utilisation de scipy.sparse (lil_matrix, csr_matrix)
- **Batch processing:** √âvaluation par lots de 500 utilisateurs
- **Caching:** Pr√©-calcul des statistiques de popularit√©
- **Limitation candidats:** Maximum 500 articles √† scorer par utilisateur

### Qualit√©

- **Filtrage intelligent:** Exclusion des articles d√©j√† vus
- **Pond√©ration temporelle:** Articles r√©cents privil√©gi√©s
- **Diversification:** Mix articles populaires + al√©atoires

### Scalabilit√©

- **Indexation efficace:** Mappings user_id ‚Üî user_idx optimis√©s
- **Calcul parall√©lisable:** ALS permet parall√©lisation native
- **M√©moire optimis√©e:** Stockage sparse pour matrices creuses

---

## Guide d'Utilisation

### Installation des d√©pendances
```bash
pip install pandas numpy scikit-learn scipy
pip install surprise implicit
pip install matplotlib seaborn
```

### Structure des donn√©es requises
```
data/
‚îú‚îÄ‚îÄ articles_metadata.csv       # M√©tadonn√©es des articles
‚îú‚îÄ‚îÄ clicks/                      # Interactions utilisateur-article
‚îÇ   ‚îî‚îÄ‚îÄ clicks_hour_*.csv
```

### Utilisation basique

```python
# Pr√©paration des donn√©es
train_df, test_df = improved_temporal_split(interactions)

# SVD
svd_rec = ImprovedSVDRecommender(n_factors=100)
svd_rec.fit(train_df)
recommendations = svd_rec.recommend(user_id, n=10)

# NMF
nmf_rec = ImprovedNMFRecommender(n_factors=50)
nmf_rec.fit(train_df)
recommendations = nmf_rec.recommend(user_id, n=10)

# ALS
als_rec = ImprovedALSRecommender(factors=100)
als_rec.fit(train_df)
recommendations = als_rec.recommend(user_id, n=10)
```

### Param√®tres ajustables

**SVD:**
- `n_factors`: [50, 100, 200] - Nombre de facteurs latents
- `n_epochs`: [10, 20, 50] - It√©rations d'entra√Ænement
- `lr_all`: [0.001, 0.005, 0.01] - Taux d'apprentissage
- `reg_all`: [0.01, 0.02, 0.05] - R√©gularisation

**NMF:**
- `n_factors`: [15, 30, 50] - Facteurs (moins pour interpr√©tabilit√©)
- `n_epochs`: [30, 50, 100] - Plus d'it√©rations n√©cessaires

**ALS:**
- `factors`: [50, 100, 200] - Dimensions latentes
- `regularization`: [0.001, 0.01, 0.1] - R√©gularisation L2
- `iterations`: [10, 15, 20] - Alternances
- `alpha`: [1, 20, 40, 100] - Param√®tre de confiance critique

---

## Consid√©rations pour la Production

### Architecture Recommand√©e
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Load Balancer ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   SVD    ‚îÇ      ‚îÇ   NMF    ‚îÇ      ‚îÇ   ALS    ‚îÇ
   ‚îÇ Service  ‚îÇ      ‚îÇ Service  ‚îÇ      ‚îÇ Service  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                  ‚îÇ                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Redis Cache  ‚îÇ
                    ‚îÇ   (15 min)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Monitoring

**M√©triques cl√©s:**
- Latence P95 < 200ms (calcul matriciel)
- Hit rate du cache > 80%
- Coverage hebdomadaire > 10%
- Diversit√© des recommandations > 0.3

**Alertes:**
- D√©gradation hit rate > 20%
- Latence spike > 500ms
- Erreurs de pr√©diction > 5%

### A/B Testing

Configuration sugg√©r√©e:
- 40% ALS (champion actuel)
- 30% ALS avec hyperparam√®tres optimis√©s
- 20% Hybride ALS + popularit√©
- 10% Contr√¥le (popularit√© pure)

M√©triques de d√©cision:
- CTR (Click-Through Rate)
- Temps de session
- Articles uniques cliqu√©s
- R√©tention J+1

### Am√©liorations Futures

1. **Deep Learning:**
   - Neural Collaborative Filtering (NCF)
   - AutoEncoders pour r√©duction dimensionnalit√©
   - Transformers pour s√©quences de clics

2. **Features additionnelles:**
   - M√©tadonn√©es articles (cat√©gorie, publisher)
   - Contexte temporel (heure, jour)
   - Signaux sociaux (partages, likes)

3. **Optimisations:**
   - Apprentissage en ligne (online learning)
   - Mise √† jour incr√©mentale des mod√®les
   - Quantification pour r√©duction m√©moire

4. **Hybridation:**
   - Ensemble learning des 3 mod√®les
   - Pond√©ration dynamique selon contexte
   - Fusion avec content-based filtering

---

## Diagnostic des Probl√®mes

### SVD avec 0% de hits

**Causes possibles:**
1. Hyperparam√®tres inadapt√©s (learning rate trop √©lev√©)
2. Overfitting sur le train
3. Probl√®me de normalisation des ratings

**Solutions:**
- Grid search sur hyperparam√®tres
- Cross-validation pour r√©gularisation optimale
- V√©rifier la distribution des ratings

### Performance ALS sup√©rieure

**Raisons:**
1. Con√ßu sp√©cifiquement pour feedback implicite
2. Gestion native de la sparsit√©
3. Param√®tre Œ± bien ajust√© pour les clics

### M√©triques globalement faibles

**Explications:**
1. Nature √©ph√©m√®re du contenu news
2. Sparsit√© extr√™me (99.8%)
3. Peu de re-clics sur m√™me article
4. Split temporel strict

---

## Conclusion

Le syst√®me de collaborative filtering impl√©ment√© offre trois approches compl√©mentaires pour la recommandation d'articles. **ALS √©merge comme le mod√®le le plus performant** avec un Hit@20 de 25.6%, particuli√®rement adapt√© aux donn√©es implicites de clics. Bien que les m√©triques absolues restent modestes, elles sont coh√©rentes avec les d√©fis inh√©rents aux recommandations d'actualit√©s (contenu √©ph√©m√®re, pas de re-consultation).

Pour la production, une approche hybride combinant ALS pour la personnalisation et un fallback sur la popularit√© temporelle est recommand√©e. L'optimisation continue des hyperparam√®tres et l'enrichissement avec des features contextuelles constituent les axes prioritaires d'am√©lioration.