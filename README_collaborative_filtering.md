# SystÃ¨me de Recommandation Collaborative Filtering

## Vue d'ensemble

Ce projet implÃ©mente trois algorithmes de filtrage collaboratif (Collaborative Filtering) pour recommander des articles d'actualitÃ©. Le systÃ¨me exploite les interactions utilisateur-article pour identifier des patterns de prÃ©fÃ©rences et gÃ©nÃ©rer des recommandations personnalisÃ©es basÃ©es sur les comportements collectifs.

## Architecture des Approches

### ğŸ¯ Approche 1: SVD (Singular Value Decomposition)

**Principe:** DÃ©composition matricielle par factorisation en valeurs singuliÃ¨res pour capturer les facteurs latents des prÃ©fÃ©rences utilisateurs et caractÃ©ristiques des articles.

**Fonctionnement:**
1. **Factorisation matricielle:** DÃ©compose la matrice R (utilisateur-article) en trois matrices
2. **Optimisation:** Minimisation de l'erreur de reconstruction avec rÃ©gularisation
3. **PrÃ©diction:** Calcul du produit des facteurs latents pour estimer les ratings manquants

**Formulation mathÃ©matique:**

Variables :
- `R` : Matrice de ratings utilisateur-article
- `U` : Matrice des facteurs latents utilisateurs (n_users Ã— n_factors)
- `Î£` : Matrice diagonale des valeurs singuliÃ¨res
- `V^T` : Matrice des facteurs latents articles (n_factors Ã— n_items)
- `p_u` : Vecteur de facteurs latents pour l'utilisateur u
- `q_i` : Vecteur de facteurs latents pour l'article i
- `Î¼` : Moyenne globale des ratings
- `b_u` : Biais de l'utilisateur u
- `b_i` : Biais de l'article i
- `Î»` : ParamÃ¨tre de rÃ©gularisation (reg_all = 0.02)
- `Î±` : Taux d'apprentissage (lr_all = 0.005)

Ã‰quations :
```
# DÃ©composition SVD
R â‰ˆ U Ã— Î£ Ã— V^T

# PrÃ©diction de rating
rÌ‚_ui = Î¼ + b_u + b_i + p_u Â· q_i

# Fonction objectif Ã  minimiser
L = Î£(r_ui - rÌ‚_ui)Â² + Î»(||p_u||Â² + ||q_i||Â² + b_uÂ² + b_iÂ²)

# Mise Ã  jour par descente de gradient
p_u â† p_u + Î±(e_ui Ã— q_i - Î» Ã— p_u)
q_i â† q_i + Î±(e_ui Ã— p_u - Î» Ã— q_i)
b_u â† b_u + Î±(e_ui - Î» Ã— b_u)
b_i â† b_i + Î±(e_ui - Î» Ã— b_i)

oÃ¹ e_ui = r_ui - rÌ‚_ui (erreur de prÃ©diction)
```

**ParamÃ¨tres du modÃ¨le:**
- `n_factors`: 100 (dimensions des facteurs latents)
- `n_epochs`: 20 (nombre d'itÃ©rations)
- `lr_all`: 0.005 (taux d'apprentissage)
- `reg_all`: 0.02 (rÃ©gularisation L2)

**Avantages:**
- Capture efficacement les patterns complexes dans les donnÃ©es
- Gestion robuste des donnÃ©es manquantes
- Bonne gÃ©nÃ©ralisation grÃ¢ce Ã  la rÃ©gularisation

**Limitations:**
- ProblÃ¨me de dÃ©marrage Ã  froid pour nouveaux utilisateurs/articles
- Sensible aux hyperparamÃ¨tres
- CoÃ»t computationnel Ã©levÃ© pour grandes matrices

---

### ğŸ§  Approche 2: NMF (Non-Negative Matrix Factorization)

**Principe:** Factorisation matricielle non-nÃ©gative garantissant l'interprÃ©tabilitÃ© des facteurs latents en imposant des contraintes de positivitÃ©.

**Fonctionnement:**
1. **DÃ©composition non-nÃ©gative:** Factorise R en deux matrices non-nÃ©gatives
2. **Optimisation itÃ©rative:** RÃ¨gles de mise Ã  jour multiplicatives
3. **InterprÃ©tation:** Les facteurs reprÃ©sentent des thÃ¨mes/catÃ©gories positifs

**Formulation mathÃ©matique:**

Variables :
- `W` : Matrice des profils utilisateurs (n_users Ã— n_factors), W â‰¥ 0
- `H` : Matrice des profils articles (n_factors Ã— n_items), H â‰¥ 0
- `w_u` : Vecteur profil de l'utilisateur u
- `h_i` : Vecteur profil de l'article i
- `n_factors` : Nombre de facteurs latents (50)
- `Îµ` : Petite constante pour Ã©viter la division par zÃ©ro

Ã‰quations :
```
# Approximation NMF
R â‰ˆ W Ã— H
avec contrainte : W â‰¥ 0, H â‰¥ 0

# PrÃ©diction
rÌ‚_ui = w_u Â· h_i = Î£_k(w_uk Ã— h_ki)

# Fonction objectif (divergence de Kullback-Leibler)
D_KL = Î£_ui[r_ui Ã— log(r_ui/rÌ‚_ui) - r_ui + rÌ‚_ui]

# RÃ¨gles de mise Ã  jour multiplicatives
W_uk â† W_uk Ã— [Î£_i(R_ui Ã— H_ki / rÌ‚_ui)] / [Î£_i H_ki]
H_ki â† H_ki Ã— [Î£_u(R_ui Ã— W_uk / rÌ‚_ui)] / [Î£_u W_uk]

# Gestion des utilisateurs/articles inconnus
Si utilisateur inconnu : utiliser moyenne des ratings de l'article
Si article inconnu : score basÃ© sur popularitÃ©
```

**ParamÃ¨tres du modÃ¨le:**
- `n_factors`: 50 (dimensions rÃ©duites pour meilleure interprÃ©tabilitÃ©)
- `n_epochs`: 50 (plus d'itÃ©rations pour convergence)

**Avantages:**
- Facteurs interprÃ©tables (toujours positifs)
- AdaptÃ© aux donnÃ©es de comptage (clics)
- RÃ©sultats explicables

**Limitations:**
- Contrainte de non-nÃ©gativitÃ© peut limiter l'expressivitÃ©
- Convergence plus lente que SVD
- Performance gÃ©nÃ©ralement infÃ©rieure Ã  SVD

---

### âš¡ Approche 3: ALS (Alternating Least Squares)

**Principe:** Optimisation alternÃ©e des facteurs utilisateurs et articles pour les donnÃ©es implicites (clics), particuliÃ¨rement efficace pour les matrices creuses.

**Fonctionnement:**
1. **Feedback implicite:** Conversion des clics en niveaux de confiance
2. **Optimisation alternÃ©e:** Fixe U pour optimiser V, puis fixe V pour optimiser U
3. **PondÃ©ration:** Les clics multiples indiquent une prÃ©fÃ©rence plus forte

**Formulation mathÃ©matique:**

Variables :
- `C_ui` : Matrice de confiance (basÃ©e sur le nombre de clics)
- `P_ui` : Matrice de prÃ©fÃ©rence binaire (1 si cliquÃ©, 0 sinon)
- `X_u` : Facteurs latents utilisateur
- `Y_i` : Facteurs latents article
- `Î±` : ParamÃ¨tre de confiance (40)
- `Î»` : RÃ©gularisation (0.01)
- `r_ui` : Nombre de clics utilisateur u sur article i

Ã‰quations :
```
# Matrice de confiance
C_ui = 1 + Î± Ã— r_ui

# Matrice de prÃ©fÃ©rence
P_ui = 1 si r_ui > 0, 0 sinon

# Fonction objectif
L = Î£_ui C_ui(P_ui - X_u^T Ã— Y_i)Â² + Î»(||X_u||Â² + ||Y_i||Â²)

# Optimisation alternÃ©e (fermÃ©e analytiquement)
Fixe Y, rÃ©sout pour X:
X_u = (Y^T Ã— C^u Ã— Y + Î»I)^(-1) Ã— Y^T Ã— C^u Ã— P^u

Fixe X, rÃ©sout pour Y:
Y_i = (X^T Ã— C^i Ã— X + Î»I)^(-1) Ã— X^T Ã— C^i Ã— P^i

oÃ¹ C^u est la matrice diagonale des confiances pour l'utilisateur u

# Score de recommandation
score(u,i) = X_u^T Ã— Y_i
```

**ParamÃ¨tres du modÃ¨le:**
- `factors`: 100 (dimensions des facteurs latents)
- `regularization`: 0.01 (rÃ©gularisation L2)
- `iterations`: 20 (nombre d'alternances)
- `alpha`: 40 (paramÃ¨tre de confiance)

**ImplÃ©mentation avec matrices creuses:**
```python
# Utilisation de lil_matrix pour construction efficace
user_item_matrix = lil_matrix((n_users, n_items))
# Conversion en csr_matrix pour calculs rapides
user_item_matrix = user_item_matrix.tocsr()
item_user_matrix = user_item_matrix.T.tocsr()
```

**Avantages:**
- TrÃ¨s efficace pour donnÃ©es implicites
- Excellent pour matrices trÃ¨s creuses (~99.8% de sparsitÃ©)
- Solution analytique Ã  chaque Ã©tape
- Meilleure performance empirique du notebook

**Limitations:**
- Suppose linÃ©aritÃ© des interactions
- NÃ©cessite tuning du paramÃ¨tre Î±
- Cold start problem persistant

---

## PrÃ©paration des DonnÃ©es

### Traitement des Interactions

**Calcul du score d'engagement:**
```python
# Score composite basÃ© sur clics et temps
max_time = interactions['avg_time_to_click'].max()
engagement_score = click_count Ã— (1 - avg_time_to_click / max_time)

# Normalisation en rating 1-5
rating = 1 + 4 Ã— (engagement_score - min) / (max - min)
```

**Statistiques du dataset:**
- Articles totaux: 363,946
- Articles cliquÃ©s: 20,793
- Utilisateurs uniques: 210,991
- Interactions totales: 1,176,699
- Rating moyen: 1.23
- SparsitÃ© de la matrice: ~99.8%

### Split Temporel AmÃ©liorÃ©

**StratÃ©gie de split:**
```python
def improved_temporal_split(interactions, test_ratio=0.2):
    # Pour chaque utilisateur:
    # - 80% premiÃ¨res interactions â†’ train
    # - 20% derniÃ¨res interactions â†’ test
    # - Garantit min 2 interactions dans train
    # - Filtre test pour articles prÃ©sents dans train
```

**RÃ©sultats du split:**
- Train: 618,118 interactions
- Test: 208,472 interactions
- Utilisateurs dans test: 78,500
- Articles testables: 3,493

---

## MÃ©triques d'Ã‰valuation

### MÃ©triques UtilisÃ©es

**Hit@K:** Utilisateur a au moins une bonne recommandation dans le TOP K
```
Hit@K = 1 si |reco[:K] âˆ© actual| > 0, 0 sinon
```

**Precision@K:** Proportion de recommandations correctes
```
Precision@K = |reco[:K] âˆ© actual| / K
```

**Recall@K:** Proportion d'articles pertinents retrouvÃ©s
```
Recall@K = |reco[:K] âˆ© actual| / |actual|
```

**F1@K:** Moyenne harmonique precision-recall
```
F1@K = 2 Ã— (Precision@K Ã— Recall@K) / (Precision@K + Recall@K)
```

### RÃ©sultats ExpÃ©rimentaux

| ModÃ¨le | Hit@5 | Hit@10 | Hit@20 | Precision@10 | Recall@10 | F1@10 |
|--------|-------|--------|--------|--------------|-----------|-------|
| **SVD** | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.0000 |
| **NMF** | 0.004 | 0.018 | 0.046 | 0.002 | 0.008 | 0.0032 |
| **ALS** | 0.064 | 0.190 | 0.256 | 0.020 | 0.091 | 0.0328 |

**Meilleur modÃ¨le:** ALS (F1@10 = 0.0328)

### Analyse des RÃ©sultats

**Performance par modÃ¨le:**
- **ALS:** 25.6% des utilisateurs ont au moins un hit @20
- **NMF:** 4.6% des utilisateurs ont au moins un hit @20
- **SVD:** 0% de hits (problÃ¨me potentiel de configuration)

**Insights:**
- Les mÃ©triques faibles reflÃ¨tent la nature Ã©phÃ©mÃ¨re des articles d'actualitÃ©
- ALS surperforme grÃ¢ce Ã  sa gestion native du feedback implicite
- SVD nÃ©cessite probablement un ajustement d'hyperparamÃ¨tres
- La sparsitÃ© extrÃªme (99.8%) impacte tous les modÃ¨les

---

## Framework d'Ã‰valuation Robuste

### Gestion des Cas Limites

```python
class ImprovedRecommender:
    def recommend(self, user_id, n=10):
        # Cas 1: Utilisateur et article connus
        if user_known and item_known:
            return model.predict(user_id, item_id)

        # Cas 2: Utilisateur inconnu, article connu
        elif item_known:
            return global_mean + item_bias

        # Cas 3: Article inconnu
        else:
            return popularity_based_score
```

### StratÃ©gies de Fallback

1. **Ã‰chantillonnage intelligent des candidats:**
   - Top 250 articles populaires
   - 250 articles alÃ©atoires
   - Limite Ã  500 candidats pour performance

2. **Scoring hybride pour articles inconnus:**
   ```python
   popularity_score = item_clicks / max_clicks
   fallback_score = global_mean Ã— (0.5 + 0.5 Ã— popularity_score)
   ```

3. **Recommandations de secours:**
   - Utilisateurs sans historique â†’ articles populaires
   - Erreurs de prÃ©diction â†’ fallback vers popularitÃ©

---

## Optimisations ImplÃ©mentÃ©es

### Performance

- **Matrices creuses:** Utilisation de scipy.sparse (lil_matrix, csr_matrix)
- **Batch processing:** Ã‰valuation par lots de 500 utilisateurs
- **Caching:** PrÃ©-calcul des statistiques de popularitÃ©
- **Limitation candidats:** Maximum 500 articles Ã  scorer par utilisateur

### QualitÃ©

- **Filtrage intelligent:** Exclusion des articles dÃ©jÃ  vus
- **PondÃ©ration temporelle:** Articles rÃ©cents privilÃ©giÃ©s
- **Diversification:** Mix articles populaires + alÃ©atoires

### ScalabilitÃ©

- **Indexation efficace:** Mappings user_id â†” user_idx optimisÃ©s
- **Calcul parallÃ©lisable:** ALS permet parallÃ©lisation native
- **MÃ©moire optimisÃ©e:** Stockage sparse pour matrices creuses

---

## Guide d'Utilisation

### Installation des dÃ©pendances
```bash
pip install pandas numpy scikit-learn scipy
pip install surprise implicit
pip install matplotlib seaborn
```

### Structure des donnÃ©es requises
```
data/
â”œâ”€â”€ articles_metadata.csv       # MÃ©tadonnÃ©es des articles
â”œâ”€â”€ clicks/                      # Interactions utilisateur-article
â”‚   â””â”€â”€ clicks_hour_*.csv
```

### Utilisation basique

```python
# PrÃ©paration des donnÃ©es
train_df, test_df = improved_temporal_split(interactions)

# SVD
svd_rec = ImprovedSVDRecommender(n_factors=100)
svd_rec.fit(train_df)
recommendations = svd_rec.recommend(user_id, n=10)

# NMF
nmf_rec = ImprovedNMFRecommender(n_factors=50)
nmf_rec.fit(train_df)
recommendations = nmf_rec.recommend(user_id, n=10)

# ALS
als_rec = ImprovedALSRecommender(factors=100)
als_rec.fit(train_df)
recommendations = als_rec.recommend(user_id, n=10)
```

### ParamÃ¨tres ajustables

**SVD:**
- `n_factors`: [50, 100, 200] - Nombre de facteurs latents
- `n_epochs`: [10, 20, 50] - ItÃ©rations d'entraÃ®nement
- `lr_all`: [0.001, 0.005, 0.01] - Taux d'apprentissage
- `reg_all`: [0.01, 0.02, 0.05] - RÃ©gularisation

**NMF:**
- `n_factors`: [15, 30, 50] - Facteurs (moins pour interprÃ©tabilitÃ©)
- `n_epochs`: [30, 50, 100] - Plus d'itÃ©rations nÃ©cessaires

**ALS:**
- `factors`: [50, 100, 200] - Dimensions latentes
- `regularization`: [0.001, 0.01, 0.1] - RÃ©gularisation L2
- `iterations`: [10, 15, 20] - Alternances
- `alpha`: [1, 20, 40, 100] - ParamÃ¨tre de confiance critique

---

## ConsidÃ©rations pour la Production

### Architecture RecommandÃ©e
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Load Balancer â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚   SVD    â”‚      â”‚   NMF    â”‚      â”‚   ALS    â”‚
   â”‚ Service  â”‚      â”‚ Service  â”‚      â”‚ Service  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Redis Cache  â”‚
                    â”‚   (15 min)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Monitoring

**MÃ©triques clÃ©s:**
- Latence P95 < 200ms (calcul matriciel)
- Hit rate du cache > 80%
- Coverage hebdomadaire > 10%
- DiversitÃ© des recommandations > 0.3

**Alertes:**
- DÃ©gradation hit rate > 20%
- Latence spike > 500ms
- Erreurs de prÃ©diction > 5%

### A/B Testing

Configuration suggÃ©rÃ©e:
- 40% ALS (champion actuel)
- 30% ALS avec hyperparamÃ¨tres optimisÃ©s
- 20% Hybride ALS + popularitÃ©
- 10% ContrÃ´le (popularitÃ© pure)

MÃ©triques de dÃ©cision:
- CTR (Click-Through Rate)
- Temps de session
- Articles uniques cliquÃ©s
- RÃ©tention J+1

### AmÃ©liorations Futures

1. **Deep Learning:**
   - Neural Collaborative Filtering (NCF)
   - AutoEncoders pour rÃ©duction dimensionnalitÃ©
   - Transformers pour sÃ©quences de clics

2. **Features additionnelles:**
   - MÃ©tadonnÃ©es articles (catÃ©gorie, publisher)
   - Contexte temporel (heure, jour)
   - Signaux sociaux (partages, likes)

3. **Optimisations:**
   - Apprentissage en ligne (online learning)
   - Mise Ã  jour incrÃ©mentale des modÃ¨les
   - Quantification pour rÃ©duction mÃ©moire

4. **Hybridation:**
   - Ensemble learning des 3 modÃ¨les
   - PondÃ©ration dynamique selon contexte
   - Fusion avec content-based filtering

---

## Diagnostic des ProblÃ¨mes

### SVD avec 0% de hits

**Causes possibles:**
1. HyperparamÃ¨tres inadaptÃ©s (learning rate trop Ã©levÃ©)
2. Overfitting sur le train
3. ProblÃ¨me de normalisation des ratings

**Solutions:**
- Grid search sur hyperparamÃ¨tres
- Cross-validation pour rÃ©gularisation optimale
- VÃ©rifier la distribution des ratings

### Performance ALS supÃ©rieure

**Raisons:**
1. ConÃ§u spÃ©cifiquement pour feedback implicite
2. Gestion native de la sparsitÃ©
3. ParamÃ¨tre Î± bien ajustÃ© pour les clics

### MÃ©triques globalement faibles

**Explications:**
1. Nature Ã©phÃ©mÃ¨re du contenu news
2. SparsitÃ© extrÃªme (99.8%)
3. Peu de re-clics sur mÃªme article
4. Split temporel strict

---

## Conclusion

Le systÃ¨me de collaborative filtering implÃ©mentÃ© offre trois approches complÃ©mentaires pour la recommandation d'articles. **ALS Ã©merge comme le modÃ¨le le plus performant** avec un Hit@20 de 25.6%, particuliÃ¨rement adaptÃ© aux donnÃ©es implicites de clics. Bien que les mÃ©triques absolues restent modestes, elles sont cohÃ©rentes avec les dÃ©fis inhÃ©rents aux recommandations d'actualitÃ©s (contenu Ã©phÃ©mÃ¨re, pas de re-consultation).

Pour la production, une approche hybride combinant ALS pour la personnalisation et un fallback sur la popularitÃ© temporelle est recommandÃ©e. L'optimisation continue des hyperparamÃ¨tres et l'enrichissement avec des features contextuelles constituent les axes prioritaires d'amÃ©lioration.